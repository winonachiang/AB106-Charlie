{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connects to MongoDB successfully!!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    client = MongoClient('127.0.0.1', 27017)\n",
    "    db = client['crawledurls'] #連結到mongoDB裡的database\n",
    "    collection = db['crawledurls'] #連結到該database的table\n",
    "    print(\"Connects to MongoDB successfully!!\")\n",
    "except:\n",
    "    print('Cannot connect to MongoDB server!')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\CF_NB.CF.000\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\CF_NBC~1.000\\AppData\\Local\\Temp\\jieba.u59a467f98f36d7ead75ba056be89879f.cache\n",
      "Loading model cost 2.5438077449798584 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\CF_NB.CF.000\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\CF_NBC~1.000\\AppData\\Local\\Temp\\jieba.u59a467f98f36d7ead75ba056be89879f.cache\n",
      "Loading model cost 2.4797613620758057 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\CF_NB.CF.000\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\CF_NBC~1.000\\AppData\\Local\\Temp\\jieba.u59a467f98f36d7ead75ba056be89879f.cache\n",
      "Loading model cost 2.7009201049804688 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\CF_NB.CF.000\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\CF_NBC~1.000\\AppData\\Local\\Temp\\jieba.u59a467f98f36d7ead75ba056be89879f.cache\n",
      "Loading model cost 2.6478826999664307 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\CF_NB.CF.000\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\CF_NBC~1.000\\AppData\\Local\\Temp\\jieba.u59a467f98f36d7ead75ba056be89879f.cache\n",
      "Loading model cost 2.5668232440948486 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "#Get contents of articles and insert them into mongoDB\n",
    "#newspaper3k 版本\n",
    "#現階段已可較正確的處理newspapaer3k抓取的文章，目前要著手處理無法用newspaper3k抓取的文章\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from newspaper import Article\n",
    "from urllib.request import urlopen\n",
    "from collections import Counter\n",
    "import time\n",
    "import datetime\n",
    "import hashlib\n",
    "datetime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#datetime記得要用成全域變數，否則會出現local variable referenced before assignment 的 error         \n",
    "start = time.time()\n",
    "end = time.time()\n",
    "elapsedTime = end - start\n",
    "\n",
    "def getArticle(url):\n",
    "    art_dict = {} #把文章內容存成一個 dictionary\n",
    "    art = Article(url, language = \"zh\")\n",
    "    try:\n",
    "        art.download() #載入文章\n",
    "        art.parse() #從文字上分析\n",
    "    except:\n",
    "        print('cannot download texts!!')\n",
    "        pass\n",
    "    art_text = art.text.replace('\\n', '').replace('\\'', '').replace('\\\"', '').replace(\"\\xa0\", \"\").replace(\"\\u3000\", \"\").replace(\"\\xa06\", \"\").replace(\"\\xa03\", \"\").replace(\"\\xa02\", \"\").replace(\"\\xa01\", \"\").strip()#word_cnt.most_common()[:10]                                                                                                                                                                                                #文章內文，後續replace是為了去除一些不需要的字元                                                                 \n",
    "    art_url = art.url.replace(\"\\n\", \"\")   #原始的url  \n",
    "    unixtime = int(time.time())  #標記時間戳(系統時間)   \n",
    "    hash_object = hashlib.md5(art_url.encode())\n",
    "    MD5code = hash_object.hexdigest()  #將url轉成MD5並且存取下來\n",
    "    jieba.set_dictionary('./dict.txt.big.txt')  #結巴斷詞字庫\n",
    "    words = jieba.cut(art_text, cut_all=False) \n",
    "    words = jieba.analyse.set_stop_words('./stopwords.txt')  \n",
    "    words = jieba.analyse.extract_tags(art_text,10)  #利用jieba.analyse來計算文本TF-IDF關鍵詞\n",
    "    #words_filtered = [word for word in cutsentence if word not in stopwords] \n",
    "    #words_filtered = str(words_filtered)\n",
    "    art_dict[\"content\"] = art_text\n",
    "    art_dict[\"url\"] = art_url\n",
    "    art_dict[\"unix_time\"] = unixtime\n",
    "    art_dict[\"datetime\"] = datetime #文章爬取時間(year-month-date hour-minute-second)\n",
    "    art_dict[\"MD5\"] = MD5code\n",
    "    art_dict[\"TF-IDF words list\"] = str(words).replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    \n",
    "    return art_dict\n",
    "\n",
    "f = open('C:\\\\Users\\\\CF_NB.CF.000\\\\uncrawledurls(not crawled)\\\\managertodayTest.txt','rt')\n",
    "lines = f.readlines()\n",
    "#print(lines).replace(\"\\n\", \" \")\n",
    "for i in lines:\n",
    "    #urlopen(i)#.decode(\"utf-8\")\n",
    "    getArticle(i)\n",
    "    \n",
    "#getArticle(\"https://www.babyou.com/opencms/channel4/Article001688.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定位爬蟲版本\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "datetime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def getTechbang(url):\n",
    "\n",
    "    res = requests.get(url)\n",
    "    res.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "\n",
    "    articles_dict = {}\n",
    "    contents = soup.select('#article-content > p')\n",
    "    contents_dict = {}\n",
    "    for page in range(len(contents)):\n",
    "        contents_dict[page+1] = contents[page].text.replace('\\n', '').replace('\\'', '').replace('\\\"', '').replace(\"\\xa0\", \"\").replace(\"\\u3000\", \"\").replace(\"\\xa06\", \"\").replace(\"\\xa03\", \"\").replace(\"\\xa02\", \"\").replace(\"\\xa01\", \"\").strip()   \n",
    "    unixtime = int(time.time())  #標記時間戳(系統時間)   \n",
    "    hash_object = hashlib.md5(url.encode())\n",
    "    MD5code = hash_object.hexdigest()  #將url轉成MD5並且存取下來\n",
    "    jieba.set_dictionary('./dict.txt.big.txt')  #結巴斷詞字庫\n",
    "    words = jieba.cut(contents_dict, cut_all=False) \n",
    "    words = jieba.analyse.set_stop_words('./stopwords.txt')  \n",
    "    words = jieba.analyse.extract_tags(str(contents_dict),10)  #利用jieba.analyse來計算文本TF-IDF關鍵詞\n",
    "\n",
    "    articles_dict['url'] = url.replace(\"\\n\", \"\")\n",
    "    articles_dict['MD5'] = MD5code\n",
    "    articles_dict['unix_time'] = unixtime\n",
    "    articles_dict['datetime'] = datetime\n",
    "    articles_dict[\"TF-IDF words list\"] = str(words).replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    articles_dict['contents'] = str(contents_dict).replace(\"{\", \"\").replace(\"}\", \"\").replace(\"1: \", \"\").replace(\"2: \", \"\").replace(\"3: \", \"\").replace(\"4: \", \"\").replace(\"5: \", \"\").replace(\"6: \", \"\").replace(\"7: \", \"\").replace(\"8: \", \"\").replace(\"9: \", \"\").replace(\"10: \", \"\").replace(\"11: \", \"\").replace(\"12: \", \"\").replace(\"13: \", \"\").replace(\"14: \", \"\").replace(\"15: \", \"\").replace(\"16: \", \"\").replace(\"17: \", \"\").replace(\"18: \", \"\").replace(\"19: \", \"\").replace(\"20: \", \"\").replace(\"21: \", \"\").replace(\"22: \", \"\").replace(\"23: \", \"\").replace(\"24: \", \"\").replace(\"25: \", \"\").replace(\"26: \", \"\").replace(\"'',\", \" \").replace(\"1'\", \"\").replace(\"1 \", \"\").replace(\"\\\\t\", \"\").replace(\"\\\\r\", \"\")\n",
    "    #print(articles_dict)\n",
    "\n",
    "    return articles_dict\n",
    "\n",
    "#\n",
    "f = open('C:\\\\Users\\\\CF_NB.CF.000\\\\uncrawledurls(not crawled)\\\\techbangTest.txt','rt')\n",
    "lines = f.readlines()\n",
    "#print(lines).replace(\"\\n\", \" \")\n",
    "for i in lines:\n",
    "    #urlopen(i)#.decode(\"utf-8\")\n",
    "    getTechbang(i)\n",
    "\n",
    "#getTechbang('https://www.techbang.com/posts/52078-a-recovered-lost-samsung-september-out-of-the-most-expensive-mobile-phones-in-the-history-of-its-galaxy-note8-priced-at-nearly-1000-euros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#newspaper3k版本的爬蟲輸入mongoDB\n",
    "f = open('C:\\\\Users\\\\CF_NB.CF.000\\\\uncrawledurls(k)\\\\allcombined(k).txt','rt')\n",
    "lines = f.readlines()\n",
    "\n",
    "#for i in lines:\n",
    "    #urlopen(i)\n",
    "    #getArticle(i) \n",
    "for i in lines:\n",
    "    art_dict = getArticle(i)\n",
    "    try:\n",
    "        collection.insert_one(art_dict)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------#\n",
    "\n",
    "#for i in lines:    \n",
    "    #art_dict = getArticle(i)    \n",
    "    #print('%s has %s paras'%(art_dict['title'],len(art_dict['content'])))    \n",
    "    #if(len(art_dict[\"content\"]) > 100): #字數小於100字的文章不存\n",
    "        #collection.insert_one(art_dict)\n",
    "        #print('[Insert %s paras ok!] %s'%(len(art_dict['content']),art_dict['title']))\n",
    "    #else:\n",
    "        #pass\n",
    "    #c.execute(\"\"\"UPDATE cklist SET tag = 'R' WHERE url = %s\"\"\", url) #這是原本要用在取MySQL裏頭的urls，在此先不需要用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定位爬蟲版本的輸入mongoDB\n",
    "f = open('C:\\\\Users\\\\CF_NB.CF.000\\\\uncrawledurls(old)\\\\beautimode(test).txt','rt')\n",
    "lines = f.readlines()\n",
    "\n",
    "#for i in lines:\n",
    "    #urlopen(i)\n",
    "    #getArticle(i)  \n",
    "for i in lines:    \n",
    "    art_dict = getBeautimode(i)    \n",
    "    #print('%s has %s paras'%(art_dict['title'],len(art_dict['content'])))    \n",
    "    #if(len(art_dict['content']) > 3): #字數小於100字的文章不存\n",
    "        #collection.insert_one(art_dict)\n",
    "        #print('[Insert %s paras ok!] %s'%(len(art_dict['content']),art_dict['title']))\n",
    "    #else:\n",
    "        #pass\n",
    "    #c.execute(\"\"\"UPDATE cklist SET tag = 'R' WHERE url = %s\"\"\", url) #這是原本要用在取MySQL裏頭的urls，在此先不需要用到\n",
    "    try:\n",
    "        collection.insert_one(art_dict)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
